{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrithviWxC\n",
    "\n",
    "This notebook will walk you through how to construct the model, load the weights, build the dataset, and use the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now configure the backends and torch states, including setting the seeds for the RNGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.enable_onednn_fusion(True)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using device: {torch.cuda.get_device_name()}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has approximately 2.3 billion parameters, so it requires reasonable computational resources, but it is possible to run it on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "### Variables and times\n",
    "\n",
    "With the environment ready to go, we now need to set up the task. The core model\n",
    "expects a fixed set of variables from the MERRA-2 dataset, which are prescribed\n",
    "below. The variables are comprised of surface variables, surface static\n",
    "variables, and variables at various vertical levels within the atmosphere. More\n",
    "details on the MERRA-2 dataset can be found\n",
    "[here](https://gmao.gsfc.nasa.gov/reanalysis/MERRA-2/). \n",
    "\n",
    "The MERRA-2 dataset includes data at longitudes of $-180^\\circ$ and $+180^\\circ$.\n",
    "This represents duplicate data, so we set a padding variable to remove it.\n",
    "\n",
    "The input to the core model consists of these variables at two different times.\n",
    "The time difference in hours between these samples is passed to the model and set in the input_time variable.\n",
    "\n",
    "The model's task is to predict the fixed set of variables at a target time, given the input data.\n",
    "\n",
    "For example, if the input times are 0900 and 1200, resulting in an input_time of -3, then a lead_time of 6 would result in a target time of 1800."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_vars = [\n",
    "    \"EFLUX\",\n",
    "    \"GWETROOT\",\n",
    "    \"HFLUX\",\n",
    "    \"LAI\",\n",
    "    \"LWGAB\",\n",
    "    \"LWGEM\",\n",
    "    \"LWTUP\",\n",
    "    \"PS\",\n",
    "    \"QV2M\",\n",
    "    \"SLP\",\n",
    "    \"SWGNT\",\n",
    "    \"SWTNT\",\n",
    "    \"T2M\",\n",
    "    \"TQI\",\n",
    "    \"TQL\",\n",
    "    \"TQV\",\n",
    "    \"TS\",\n",
    "    \"U10M\",\n",
    "    \"V10M\",\n",
    "    \"Z0M\",\n",
    "]\n",
    "static_surface_vars = [\"FRACI\", \"FRLAND\", \"FROCEAN\", \"PHIS\"]\n",
    "vertical_vars = [\"CLOUD\", \"H\", \"OMEGA\", \"PL\", \"QI\", \"QL\", \"QV\", \"T\", \"U\", \"V\"]\n",
    "levels = [\n",
    "    34.0,\n",
    "    39.0,\n",
    "    41.0,\n",
    "    43.0,\n",
    "    44.0,\n",
    "    45.0,\n",
    "    48.0,\n",
    "    51.0,\n",
    "    53.0,\n",
    "    56.0,\n",
    "    63.0,\n",
    "    68.0,\n",
    "    71.0,\n",
    "    72.0,\n",
    "]\n",
    "padding = {\"level\": [0, 0], \"lat\": [0, -1], \"lon\": [0, 0]}\n",
    "\n",
    "lead_times = [6]  # This varibale can be change to change the task\n",
    "input_times = [-6]  # This varibale can be change to change the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data file\n",
    "MERRA-2 data is available from 1980 to the present day, at 3-hour temporal resolution. The dataloader we have provided expects the surface data and vertical data to be saved in separate files, and when provided with the directories, will search for the relevant data that falls within the provided time range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range = (\"2016-02-01T00:00:00\", \"2016-02-06T23:59:59\")\n",
    "\n",
    "surf_dir = Path(\"path/to/merra-2\")\n",
    "vert_dir = Path(\"path/to/merra-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climatology\n",
    "The PrithviWxC model was trained to calculate the output by producing a perturbation to the climatology at the target time.\n",
    " This mode of operation is set via the `residual=climate` option. This was chosen as climatology is typically a strong prior for long-range prediction. When using the `residual=climate` option, we have to provide the dataloader with the path of the climatology data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_clim_dir = Path(\"path/to/climatology\")\n",
    "vert_clim_dir = Path(\"path/to/climatology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postion encoding\n",
    "Position data is included in the data passed to the model,  as this allows the attention mechanism to determine data locality rather than explicit or implicit data connections. The position data is encoded in the model with two possible options, fourier or absolute. As these encoding options require different treatment within the data loader, the chosen option is set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoding = \"fourier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset init\n",
    "We can now instantiate the MERRA2 Dataset class provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrithviWxC.dataloaders.merra2 import Merra2Dataset\n",
    "\n",
    "dataset = Merra2Dataset(\n",
    "    time_range=time_range,\n",
    "    lead_times=lead_times,\n",
    "    input_times=input_times,\n",
    "    data_path_surface=surf_dir,\n",
    "    data_path_vertical=vert_dir,\n",
    "    climatology_path_surface=surf_clim_dir,\n",
    "    climatology_path_vertical=vert_clim_dir,\n",
    "    surface_vars=surface_vars,\n",
    "    static_surface_vars=static_surface_vars,\n",
    "    vertical_vars=vertical_vars,\n",
    "    levels=levels,\n",
    "    positional_encoding=positional_encoding,\n",
    ")\n",
    "assert len(dataset) > 0, \"There doesn't seem to be any valid data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "We are now ready to build the mdoel.\n",
    "### Scalers\n",
    "Additionally, the model takes as static parameters the mean and variance values of the input variables and the variance values of the target difference, i.e., the variance between climatology and instantaneous variables. We have provided data files containing these values, and here we load this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrithviWxC.dataloaders.merra2 import (\n",
    "    input_scalers,\n",
    "    output_scalers,\n",
    "    static_input_scalers,\n",
    ")\n",
    "\n",
    "surf_in_scal_path = Path(\"path/to/musigma_surface.nc\")\n",
    "vert_in_scal_path = Path(\"path/to/musigma_vertical.nc\")\n",
    "surf_out_scal_path = Path(\"path/to/anomaly_variance_surface.nc\")\n",
    "vert_out_scal_path = Path(\"path/to/anomaly_variance_vertical.nc\")\n",
    "\n",
    "in_mu, in_sig = input_scalers(\n",
    "    surface_vars,\n",
    "    vertical_vars,\n",
    "    levels,\n",
    "    surf_in_scal_path,\n",
    "    vert_in_scal_path,\n",
    ")\n",
    "\n",
    "output_sig = output_scalers(\n",
    "    surface_vars,\n",
    "    vertical_vars,\n",
    "    levels,\n",
    "    surf_out_scal_path,\n",
    "    vert_out_scal_path,\n",
    ")\n",
    "\n",
    "static_mu, static_sig = static_input_scalers(\n",
    "    surf_in_scal_path,\n",
    "    static_surface_vars,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task and additional configs\n",
    "As previously mentioned, the PrithviWxC model's pretext task involved predicting the desired variable at a specific lead time. This was achieved by calculating the difference (delta) compared to the climatological average at that time. This operational mode is activated using the residual flag. Although the model includes additional residual options, the core model weights were not trained using these modes.\n",
    "\n",
    "Additionally, for training and evaluation, it is possible to mask tokens in the model. The masking occurs after tokenization, prior to the encoder layers. The model utilizes multi-axis attention, with data broken down into a hierarchy of local and global patches. Consequently, masking can be configured to mask either small local patches or larger global patches. This configuration is achieved via the `masking_mode` flag. It is possible to set `masking_mode=both`. This does not mix the modes but rather allows both modes to be used and swapped between, primarily for training purposes. For this demonstration, we will adjust the masking ratio to showcase the reconstruction capabilities of the model.\n",
    "\n",
    "Finally, we can set up shifting. Primarily utilized in the decoder, this enables alternate shifting of the attention windows, similar to the SWIN model. This option necessitates an even number of decoder blocks and is incompatible with the encoder when masking is also employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = \"climate\"\n",
    "masking_mode = \"local\"\n",
    "decoder_shifting = True\n",
    "masking_ratio = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model init\n",
    "We now have all the pieces to build the model. If you are using the pretrained weights, a number of the model hyperparameters are predetermined and included below. With this configuration, the model will have approximately 2.3 billion parameters. Therefore, if you want to train the fully unfrozen model, you will likely need to use a model distribution approach, such as fully shared data parallelism (FSDP). To further reduce the memory usage of the model when gradients are required, there are two variables — `checkpoint_encoder` and `checkpoint_decoder` — which enable activation checkpointing of desired transformer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrithviWxC.model import PrithviWxC\n",
    "\n",
    "model = PrithviWxC(\n",
    "    in_channels=160,\n",
    "    input_size_time=2,\n",
    "    in_channels_static=8,\n",
    "    input_scalers_mu=in_mu,\n",
    "    input_scalers_sigma=in_sig,\n",
    "    input_scalers_epsilon=0.0,\n",
    "    static_input_scalers_mu=static_mu,\n",
    "    static_input_scalers_sigma=static_sig,\n",
    "    static_input_scalers_epsilon=0.0,\n",
    "    output_scalers=output_sig**0.5,\n",
    "    n_lats_px=360,\n",
    "    n_lons_px=576,\n",
    "    patch_size_px=[2, 2],\n",
    "    mask_unit_size_px=[30, 32],\n",
    "    mask_ratio_inputs=masking_ratio,\n",
    "    embed_dim=2560,\n",
    "    n_blocks_encoder=12,\n",
    "    n_blocks_decoder=2,\n",
    "    mlp_multiplier=4,\n",
    "    n_heads=16,\n",
    "    dropout=0.0,\n",
    "    drop_path=0.0,\n",
    "    parameter_dropout=0.0,\n",
    "    residual=residual,\n",
    "    masking_mode=masking_mode,\n",
    "    decoder_shifting=decoder_shifting,\n",
    "    positional_encoding=positional_encoding,\n",
    "    checkpoint_encoder=[],\n",
    "    checkpoint_decoder=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights\n",
    "We have provided unshared pretrained weights for the model, which can now be loaded. The model can then be transferred to the requested device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = Path(\"path/to/step_400.pt\")\n",
    "\n",
    "state_dict = torch.load(weights_path, weights_only=False)\n",
    "if \"model_state\" in state_dict:\n",
    "    state_dict = state_dict[\"model_state\"]\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "if (hasattr(model, \"device\") and model.device != device) or not hasattr(\n",
    "    model, \"device\"\n",
    "):\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "We are now ready to perform inference on the model. The data returned from the dataset class requires additional preprocessing; therefore, after polling the dataset, we process the data using the `preproc` function. This processed data is then transferred to the device. To recover the masking, we can save the torch RNG state and use it later. Finally, we run the model in evaluation mode without generating the gradient graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrithviWxC.dataloaders.merra2 import preproc\n",
    "\n",
    "data = next(iter(dataset))\n",
    "batch = preproc([data], padding)\n",
    "\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "rng_state_1 = torch.get_rng_state()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    out = model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m = out[0, 12].cpu().numpy()\n",
    "\n",
    "lat = np.linspace(-90, 90, out.shape[-2])\n",
    "lon = np.linspace(-180, 180, out.shape[-1])\n",
    "X, Y = np.meshgrid(lon, lat)\n",
    "\n",
    "plt.contourf(X, Y, t2m, 100)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
